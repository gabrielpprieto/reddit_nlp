{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: `Web API's` & `Classification`\n",
    "---\n",
    "_Gabriel Perez Prieto_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_Data Collection and API's - Reddit\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max_columns display to 500\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Posts from Subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to scrape subreddits\n",
    "def scrape_reddit(*args, posts=500):\n",
    "    ''' \n",
    "    Scrapes Reddit's Subreddits Using PushShift's API\n",
    "        \n",
    "    Inputs = subreddits and number of posts to be requested (Multiple of 500!)\n",
    "    Output = Pandas DataFrame\n",
    "        \n",
    "    Fields Returned = author, title, num_comments, link_flair_text, url\n",
    "    '''\n",
    "    \n",
    "    # StartTime\n",
    "    start  = time.time()\n",
    "    \n",
    "    # Create and empty dictionary to house data\n",
    "    data_compiled = {}\n",
    "    \n",
    "    # Loop through function's inputs\n",
    "    for subreddit in args:\n",
    "\n",
    "        # Loop through _ times - 500/request limit \n",
    "        for i in range(0, int(posts / 500)):\n",
    "            \n",
    "            # Set condition to collect posts before previously collected posts\n",
    "            if i == 0:\n",
    "                last_post_time = ''\n",
    "            else:\n",
    "                before = last_post_time\n",
    "\n",
    "            # Print Initializing\n",
    "            print(f'Scraping {subreddit}...')\n",
    "\n",
    "            # Set max requested posts at a time - API rules!\n",
    "            rows_at_a_time = int(posts / (posts / 500))\n",
    "            \n",
    "            # Select reddit's parameters\n",
    "            base_url = 'https://api.pushshift.io/reddit/search/comment/' # or submissions for titles\n",
    "            search_term = '?q=' + '' + '&'\n",
    "            sub_reddit = 'subreddit=' + subreddit + '&'\n",
    "            fields = 'fields=created_utc,author,title,body,num_comments,url,score' + '&' #title for submissions\n",
    "            sort_type = 'num_comments' + '&'\n",
    "            sort = 'sort=desc' + '&'\n",
    "            size = 'size=' + str(rows_at_a_time) + '&'\n",
    "            before = 'before=' + str(last_post_time) + '&'               \n",
    "\n",
    "            # Set url for scraping\n",
    "            url = base_url + search_term + sub_reddit  + sort_type + sort + size + before + fields\n",
    "         \n",
    "            # Print JSON's url for each request\n",
    "            print(f'JSON {subreddit}: {url}')\n",
    "\n",
    "            # Create a request\n",
    "            res = requests.get(url)\n",
    "\n",
    "            # Request's status_code\n",
    "            status_code = res.status_code\n",
    "\n",
    "            # Print status code of subreddit being scraped\n",
    "            print(f'Request Status: {status_code}')\n",
    "\n",
    "            # Error if status code is different from 200\n",
    "            if status_code != 200:\n",
    "                print(f'Error Occurred, Request Status: {status_code}')\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Create data from JSON\n",
    "                data = res.json()['data']\n",
    "\n",
    "                # Append data from different subreddit scraped\n",
    "                data_compiled[str(subreddit)+str(i)] = data\n",
    "\n",
    "                # Print number of rows requested\n",
    "                print(f'{rows_at_a_time} Row(s) Scraped from: {subreddit}\\n')\n",
    "\n",
    "                # Set last_post time to request posts before this one on second loop\n",
    "                last_post_time = data_compiled[str(subreddit) + str(int(i))][rows_at_a_time - 1]['created_utc']\n",
    "                \n",
    "                # Wait 10 senconds for the next request\n",
    "                time.sleep(10)\n",
    "                \n",
    "    # Create empty pandas DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Loop through dictionary with list of dictionaries - Keys = subreddits, Values = data\n",
    "    for key in data_compiled.keys():\n",
    "\n",
    "        # Create pandas DataFrame for requested data\n",
    "        df_subreddit = pd.DataFrame(data_compiled[key])\n",
    "        df_subreddit['subreddit'] = key\n",
    "\n",
    "        # Concatenate DataFrames\n",
    "        df = pd.concat([df, df_subreddit], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Convert utc to datetime and create a new column with dates\n",
    "    df['created_date'] = df['created_utc'].map(lambda x: datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # Drop created_utc column - not necessary\n",
    "    df.drop(columns=['created_utc'], axis=1, inplace=True)\n",
    "    \n",
    "    # Reset subreddit columns to subreddit name - included i valuess\n",
    "    df['subreddit'] = df['subreddit'].map(lambda x: x[:len(x) - len(str(int(i)))])\n",
    "    \n",
    "    # Print total runtime including 10 sec buffer in between requests\n",
    "    print(f'Total RunTime: {time.time() - start}')\n",
    "    \n",
    "    # Return final DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run scrape_reddit Function and Save as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571328366&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571310032&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571281225&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571267413&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571255249&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571243742&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571221558&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571189532&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegan...\n",
      "JSON vegan: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegan&num_comments&sort=desc&size=500&before=1571176307&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegan\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1571177431&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1571042357&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570924099&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570786653&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570651505&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570546317&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570452433&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570297587&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Scraping vegetarian...\n",
      "JSON vegetarian: https://api.pushshift.io/reddit/search/comment/?q=&subreddit=vegetarian&num_comments&sort=desc&size=500&before=1570200555&fields=created_utc,author,title,body,num_comments,url,score&\n",
      "Request Status: 200\n",
      "500 Row(s) Scraped from: vegetarian\n",
      "\n",
      "Total RunTime: 228.6093008518219\n"
     ]
    }
   ],
   "source": [
    "df = scrape_reddit('vegan', 'vegetarian', posts=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check First 5 Rows - Make Sure Data Was Pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ThisIsMyRental</td>\n",
       "      <td>My, my, my, you've outdone yourself friend! It...</td>\n",
       "      <td>1</td>\n",
       "      <td>vegan</td>\n",
       "      <td>2019-10-17 20:01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NorthernTurnip</td>\n",
       "      <td>Yeah that drives me crazy</td>\n",
       "      <td>1</td>\n",
       "      <td>vegan</td>\n",
       "      <td>2019-10-17 20:01:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SkarKrow</td>\n",
       "      <td>London is amazing.\\n\\nThough if you visit the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>vegan</td>\n",
       "      <td>2019-10-17 20:00:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Breaking-finch</td>\n",
       "      <td>Because you took their land from them and disp...</td>\n",
       "      <td>0</td>\n",
       "      <td>vegan</td>\n",
       "      <td>2019-10-17 19:59:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>beariesad</td>\n",
       "      <td>ohh no shame in it at all! it's good for you, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>vegan</td>\n",
       "      <td>2019-10-17 19:59:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               body  score  \\\n",
       "0  ThisIsMyRental  My, my, my, you've outdone yourself friend! It...      1   \n",
       "1  NorthernTurnip                          Yeah that drives me crazy      1   \n",
       "2        SkarKrow  London is amazing.\\n\\nThough if you visit the ...      1   \n",
       "3  Breaking-finch  Because you took their land from them and disp...      0   \n",
       "4       beariesad  ohh no shame in it at all! it's good for you, ...      2   \n",
       "\n",
       "  subreddit         created_date  \n",
       "0     vegan  2019-10-17 20:01:40  \n",
       "1     vegan  2019-10-17 20:01:32  \n",
       "2     vegan  2019-10-17 20:00:57  \n",
       "3     vegan  2019-10-17 19:59:50  \n",
       "4     vegan  2019-10-17 19:59:21  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Shape and Value Counts for each Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vegetarian    5000\n",
       "vegan         5000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as .csv\n",
    "df.to_csv('./data/reddit.csv', index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
